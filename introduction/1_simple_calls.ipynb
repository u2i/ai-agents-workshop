{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451495c7",
   "metadata": {},
   "source": [
    "# How to call a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea0a51",
   "metadata": {},
   "source": [
    "```shell\n",
    "curl -N \\\n",
    "  -X POST \"http://localhost:11434/api/chat\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"llama3.2\",\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": \"Hello! Can you tell me a joke?\"}\n",
    "    ],\n",
    "    \"stream\": true\n",
    "  }'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ed79e",
   "metadata": {},
   "source": [
    "## Basic API calls using HTTP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64715f46",
   "metadata": {},
   "source": [
    "Here's a basic example of how to call an Ollama model using Python's `requests` library. Ollama runs locally on your machine and provides a simple HTTP API that doesn't require any API keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeba045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API = \"http://ollama:11434/api/chat\"\n",
    "MODEL = \"llama3.2\"\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72121c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's response:\n",
      "\n",
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "An impasta!\n",
      "\n",
      "I hope that made you smile! Do you want to hear another one?\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "Full response:\n",
      "{\n",
      "  \"model\": \"llama3.2\",\n",
      "  \"created_at\": \"2025-12-18T22:48:18.044558383Z\",\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Here's one:\\n\\nWhat do you call a fake noodle?\\n\\n(Wait for it...)\\n\\nAn impasta!\\n\\nI hope that made you smile! Do you want to hear another one?\"\n",
      "  },\n",
      "  \"done\": true,\n",
      "  \"done_reason\": \"stop\",\n",
      "  \"total_duration\": 4251103918,\n",
      "  \"load_duration\": 90055916,\n",
      "  \"prompt_eval_count\": 40,\n",
      "  \"prompt_eval_duration\": 438240084,\n",
      "  \"eval_count\": 38,\n",
      "  \"eval_duration\": 3711330627\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "data = {\n",
    "    \"model\": MODEL,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello! Can you tell me a joke?\"},\n",
    "    ],\n",
    "    \"stream\": False,\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(url=OLLAMA_API, headers=headers, data=json.dumps(data))\n",
    "    response.raise_for_status()\n",
    "\n",
    "    response_data = response.json()\n",
    "\n",
    "    content = response_data[\"message\"][\"content\"]\n",
    "    print(f\"Model's response:\\n\\n{content}\", end=\"\\n\\n\")\n",
    "\n",
    "    print(\"-\" * 32)\n",
    "    print(\"\\nFull response:\")\n",
    "    print(json.dumps(response_data, indent=2))\n",
    "\n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    print(\"\\nMake sure Ollama is running\\n\\n\")\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "969f6a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's response:\n",
      "Here's one for you:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "An impasta!\n",
      "\n",
      "Hope that made you smile! Do you want to hear another one?"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import sys\n",
    "\n",
    "data = {\n",
    "    \"model\": MODEL,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello! Can you tell me a joke?\"},\n",
    "    ],\n",
    "    \"stream\": True,\n",
    "}\n",
    "\n",
    "try:\n",
    "    with requests.post(\n",
    "        url=OLLAMA_API, headers=headers, data=json.dumps(data), stream=True\n",
    "    ) as response:\n",
    "        response.raise_for_status()\n",
    "\n",
    "        print(\"Model's response:\")\n",
    "\n",
    "        for line in response.iter_lines():\n",
    "            chunk = json.loads(line)\n",
    "\n",
    "            content = chunk[\"message\"][\"content\"]\n",
    "            print(content, end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    raise Exception(f\"\\nMake sure Ollama is running\\n\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9818f9",
   "metadata": {},
   "source": [
    "# Pydantic AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42d27b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentRunResult(output=\"Here's one:\\n\\nWhat do you call a fake noodle?\\n\\n(Wait for it...)\\n\\nAn impasta!\\n\\nHope that made you laugh! Do you want to hear another one?\")\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    model=f\"ollama:{MODEL}\",\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    ")\n",
    "\n",
    "result = await agent.run(\"Hello! Can you tell me a joke?\")\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
